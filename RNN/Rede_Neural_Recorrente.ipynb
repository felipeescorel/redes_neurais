{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rede Neural Recorrente",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vmacf/redes_neurais/blob/master/RNN/Rede_Neural_Recorrente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pwNCDReQ46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d45c26c7-2cfd-436a-c46b-c3dd4a480cb2"
      },
      "source": [
        "!git clone https://github.com/vmacf/redes_neurais"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'redes_neurais' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-SRhEcR9duk",
        "colab_type": "text"
      },
      "source": [
        "# Rede Neural Recorrente\n",
        "\n",
        "Tutorial baseado no site: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AubxSe_9duo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "9676de42-c4cd-4198-af7e-c234ed536bb4"
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igBT039J9du7",
        "colab_type": "text"
      },
      "source": [
        "## Modelagem de linguagem (Language Modeling)\n",
        "\n",
        "Nessa aula, vamos desenvolver um gerador de linguagem utilizando rede neural recorrente (RNN). \n",
        "\n",
        "Por exemplo, se temos uma sentença de $m$ palavras. A modelagem de linguagem realiza a previsão de ser observar uma determinada sentença, numa base de dados:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "P(w_1,...,w_m) = \\prod_{i=1}^{m}P(w_i \\mid w_1,..., w_{i-1}) \n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "\n",
        "Utilizando a RNN, a probabilidade de uma sentença é o produto das probabilidades de cada palavra, dadas as palavras que vieram antes dela. Assim, a probabilidade da sentença \"Ele foi comprar alguns chocolates\", seria a probabilidade de \"chocolate\" dado \"Ele foi comprar alguns\", multiplicado pela probabilidade de \"alguns\", dado \"Ele foi comprar\", e assim por diante.\n",
        "\n",
        "Por que isso é útil? Por que queremos atribuir uma probabilidade de observar uma sentença?\n",
        "\n",
        "Podemos utilizar essa modelagem pra resolver alguns problemas de processamento de linguagem natual. Por exemplo, esse modelo pode ser usado como um mecanismo de pontuação para problemas como saber qual a próxima sentença num sistema de tradução. Intuitivamente, a sentença mais provável,  é aquela que seja gramaticalmente correta. Pontuação semelhante acontece em sistemas de reconhecimento de fala. \n",
        "\n",
        "Também, é possível gerar novos textos. Dada uma sequência existente de palavras,  a próxima palavra é prevista pelas probabilidades de uma próxima palavra, e esse processo é reptido até formar uma frase completa.\n",
        "\n",
        "Note que na equação acima, a probabilidade de cada palavra é condicionada a **todas** as palavras anteriores. Na prática, muitos modelos têm dificuldade em representar essas dependências de longo prazo devido a restrições computacionais ou memória. Eles são tipicamente limitados a apenas algumas das palavras anteriores, conforme discutido na aula.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEfRJ7Id9du9",
        "colab_type": "text"
      },
      "source": [
        "## 1) Base de dados\n",
        "\n",
        "Serão utilizados 15000 comentários do reddit,  disponíveis em [dataset available on Google's BigQuery](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08). \n",
        "\n",
        "A ideia é que os textos gerados pelo modelo sejam parecidos com os comentários de usuários do reddit.\n",
        "\n",
        "##Pré-Processamento\n",
        "\n",
        "#### 1.1. Tokenize\n",
        "\n",
        "Separa o texto em tokens: A sentença \"He left!\" deve ter 3 tokens: \"He\", \"left\", \"!\". \n",
        "\n",
        "#### 1.2. Remove as palavras menos frequentes\n",
        "\n",
        "Remove as palavras que aparecem pouco nos textos, por exemplo, 1 ou 2 vezes. No código, é possível limistar o tamanho do vocabulário, pois o tamanho do vocabulário influencia na velocidade de treinamento. Todas as palavras que não estiverem no vocabulário serão trocadas por `UNKNOWN_TOKEN`. \n",
        "\n",
        "\n",
        "#### 1.3. Tokens especiais de início e fim\n",
        "\n",
        "Aprender palavras que iniciam e terminam uma sentença é importante para delimitar as sentenças. Assim, existem os tokens  `SENTENCE_START` e `SENTENCE_END` para cada frase. \n",
        "\n",
        "#### 1.4. Construir matrizes de dados de treinamento\n",
        "\n",
        "A entrada para das redes neurais recorrentes são vetores, não sequências de caracteres. Então, será realizado um mapeamento entre palavras e índices, `index_to_word` e` word_to_index`. Por exemplo, a palavra \"friendly\" pode estar no índice 2001. Um exemplo de treinamento $x$ pode ser \"[0, 179, 341, 416]\", onde 0 corresponde a \"SENTENCE_START\". O rótulo correspondente $y$ seria `[179, 341, 416, 1]`. \n",
        "\n",
        "O objetivo dessa rede é prever a próxima palavra, então $y$ é apenas o vetor $x$ deslocado por uma posição, com o último elemento sendo o símbolo `SENTENCE_END`. Em outras palavras, a predição correta para a palavra `179` é `341`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4raCw-X9du_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "bb4a3bd0-c4b6-46b6-93ed-54f436175252"
      },
      "source": [
        "vocabulary_size = 8000\n",
        "unknown_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
        "print \"Reading CSV file...\"\n",
        "with open('redes_neurais/RNN/data/reddit-comments-2015-08.csv', 'rb') as f:\n",
        "    reader = csv.reader(f, skipinitialspace=True)\n",
        "    reader.next()\n",
        "    # Split full comments into sentences\n",
        "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
        "    # Append SENTENCE_START and SENTENCE_END\n",
        "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "print \"Parsed %d sentences.\" % (len(sentences))\n",
        "    \n",
        "# Tokenize the sentences into words\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Count the word frequencies\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
        "\n",
        "# Get the most common words and build index_to_word and word_to_index vectors\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print \"Using vocabulary size %d.\" % vocabulary_size\n",
        "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
        "\n",
        "# Replace all words not in our vocabulary with the unknown token\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "print \"\\nExemplo de frase: '%s'\" % sentences[0]\n",
        "print \"\\nExemplo de frase após o pré-processamento: '%s'\" % tokenized_sentences[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading CSV file...\n",
            "Parsed 79170 sentences.\n",
            "Found 65498 unique words tokens.\n",
            "Using vocabulary size 8000.\n",
            "The least frequent word in our vocabulary is 'traction' and appeared 10 times.\n",
            "\n",
            "Exemplo de frase: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
            "\n",
            "Exemplo de frase após o pré-processamento: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJAY5oIIvy_n",
        "colab_type": "text"
      },
      "source": [
        "## 2) Cria os dados de treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt4-VUJd9dvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the training data\n",
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2kV9noM9dvK",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Exemplos de sentenças:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnWpaGLo9dvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "7436d52c-1a10-4831-dcb8-a427aac37a89"
      },
      "source": [
        "# Print an training data example\n",
        "x_example, y_example = X_train[17], y_train[17]\n",
        "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
        "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "SENTENCE_START what are n't you understanding about this ? !\n",
            "[0, 51, 27, 16, 10, 861, 54, 25, 34, 69]\n",
            "\n",
            "y:\n",
            "what are n't you understanding about this ? ! SENTENCE_END\n",
            "[51, 27, 16, 10, 861, 54, 25, 34, 69, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D8erDoc9dvQ",
        "colab_type": "text"
      },
      "source": [
        "## 3) Construção da RNN\n",
        "\n",
        "Rede neural desdobrada:\n",
        "\n",
        "![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
        "\n",
        "Funções de ativação:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "s_t &= \\tanh(Ux_t + Ws_{t-1}) \\\\\n",
        "o_t &= \\mathrm{softmax}(Vs_t)\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "Para um tamanho de vocabulário $C = 8000$ e um tamanho de camada oculta $H = 100$, teremos:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "x_t & \\in \\mathbb{R}^{8000} \\\\\n",
        "o_t & \\in \\mathbb{R}^{8000} \\\\\n",
        "s_t & \\in \\mathbb{R}^{100} \\\\\n",
        "U & \\in \\mathbb{R}^{100 \\times 8000} \\\\\n",
        "V & \\in \\mathbb{R}^{8000 \\times 100} \\\\\n",
        "W & \\in \\mathbb{R}^{100 \\times 100} \\\\\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Os pesos $U,V$ e $W$ são os parâmetros que queremos aprender. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAlLmPcF9dvS",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1) Inicialização\n",
        "\n",
        "Inicializar no intervalo $\\left[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}\\right]$ onde $n$ é o número de conexões de entrada da camada anterior. \n",
        "\n",
        "Os parâmetros `word_dim` é o tamanho do vocabulário, e` hidden_dim` é o tamanho da  camada oculta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEANChdo9dvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNNumpy:\n",
        "    \n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        # Assign instance variables\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        # Randomly initialize the network parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaC0R0Bq9dvf",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2) Forward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUX0u0049dvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)\n",
        "\n",
        "def forward_propagation(self, x):\n",
        "    # The total number of time steps\n",
        "    T = len(x)\n",
        "    # During forward propagation we save all hidden states in s because need them later.\n",
        "    # We add one additional element for the initial hidden, which we set to 0\n",
        "    s = np.zeros((T + 1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "    # The outputs at each time step. Again, we save them for later.\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    # For each time step...\n",
        "    for t in np.arange(T):\n",
        "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s]\n",
        "\n",
        "RNNNumpy.forward_propagation = forward_propagation\n",
        "\n",
        "\n",
        "def predict(self, x):\n",
        "    # Perform forward propagation and return index of the highest score\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis=1)\n",
        "\n",
        "RNNNumpy.predict = predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-i11u4B9dv9",
        "colab_type": "text"
      },
      "source": [
        "## 4) Função de perda\n",
        "\n",
        "Calcula a função de perda logarítmica. \n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} o_{n} \\log y_{n}\n",
        "\\end{aligned}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_dS1Kct9dv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "    # For each sentence...\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "        # We only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "        # Add to the loss based on how off we were\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # Divide the total loss by the number of training examples\n",
        "    N = np.sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x,y)/N\n",
        "\n",
        "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
        "RNNNumpy.calculate_loss = calculate_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euBiFF4G9dwC",
        "colab_type": "text"
      },
      "source": [
        "### 4.1) Saída pra 1000 exemplos sem treinamento:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxF8Fa1T9dwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "c68ae708-62da-4ff2-c920-6088ecbcacd0"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "\n",
        "# Limit to 1000 examples to save time\n",
        "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
        "print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected Loss for random predictions: 8.987197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actual loss: 8.987430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezxoQD1M9dwJ",
        "colab_type": "text"
      },
      "source": [
        "## 5) Treinando o RNN com SGD e Backpropagation Through Time (BPTT)\n",
        "\n",
        "O treinamento do BPTT equivale as derivadas parciais: $\\frac{\\partial E}{\\partial U}, \\frac{\\partial E}{\\partial V}, \\frac{\\partial E}{\\partial W}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqMpxJ799dwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bptt(self, x, y):\n",
        "    T = len(y)\n",
        "    # Perform forward propagation\n",
        "    o, s = self.forward_propagation(x)\n",
        "    # We accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    # For each output backwards...\n",
        "    for t in np.arange(T)[::-1]:\n",
        "        dLdV += np.outer(delta_o[t], s[t].T)\n",
        "        # Initial delta calculation\n",
        "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
        "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
        "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
        "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
        "            dLdU[:,x[bptt_step]] += delta_t\n",
        "            # Update delta for next step\n",
        "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
        "    return [dLdU, dLdV, dLdW]\n",
        "\n",
        "RNNNumpy.bptt = bptt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk3LoJCk9dwX",
        "colab_type": "text"
      },
      "source": [
        "## 6) Implementação SGD\n",
        "\n",
        "O SGD é implementado em duas etapas: 1. Uma função `sdg_step`, que calcula os gradientes e executa as atualizações para um lote; 2. Um loop externo que percorre o conjunto de treinamento e ajusta a taxa de aprendizado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tTq_A1F9dwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs one step of SGD.\n",
        "def numpy_sdg_step(self, x, y, learning_rate):\n",
        "    # Calculate the gradients\n",
        "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
        "    # Change parameters according to gradients and learning rate\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "\n",
        "RNNNumpy.sgd_step = numpy_sdg_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEn95Dt89dwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outer SGD Loop\n",
        "# - model: The RNN model instance\n",
        "# - X_train: The training data set\n",
        "# - y_train: The training data labels\n",
        "# - learning_rate: Initial learning rate for SGD\n",
        "# - nepoch: Number of times to iterate through the complete dataset\n",
        "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
        "    # We keep track of the losses so we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    for epoch in range(nepoch):\n",
        "        # Optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
        "            # Adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5  \n",
        "                print \"Setting learning rate to %f\" % learning_rate\n",
        "            sys.stdout.flush()\n",
        "        # For each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            # One SGD step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            num_examples_seen += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLgTl9-e9dwn",
        "colab_type": "text"
      },
      "source": [
        "### 6.1)Treino com o SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibI4juzw9dwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "a543fd95-cfa6-42fd-d965-c1a9d353bbec"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-14 17:56:10: Loss after num_examples_seen=0 epoch=0: 8.987468\n",
            "2019-05-14 17:56:24: Loss after num_examples_seen=100 epoch=1: 8.976080\n",
            "2019-05-14 17:56:38: Loss after num_examples_seen=200 epoch=2: 8.959668\n",
            "2019-05-14 17:56:52: Loss after num_examples_seen=300 epoch=3: 8.928843\n",
            "2019-05-14 17:57:06: Loss after num_examples_seen=400 epoch=4: 8.739638\n",
            "2019-05-14 17:57:20: Loss after num_examples_seen=500 epoch=5: 6.670565\n",
            "2019-05-14 17:57:33: Loss after num_examples_seen=600 epoch=6: 6.212330\n",
            "2019-05-14 17:57:47: Loss after num_examples_seen=700 epoch=7: 5.970069\n",
            "2019-05-14 17:58:01: Loss after num_examples_seen=800 epoch=8: 5.809121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ije34s6G9dxA",
        "colab_type": "text"
      },
      "source": [
        "## 7) Geração de Texto\n",
        "\n",
        "Função que gera novas sentenças:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01pSfZQq9dxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sentence(model):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    # Repeat until we get an end token\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
        "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
        "        sampled_word = word_to_index[unknown_token]\n",
        "        # We don't want to sample unknown words\n",
        "        while sampled_word == word_to_index[unknown_token]:\n",
        "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
        "            sampled_word = np.argmax(samples)\n",
        "        new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
        "    return sentence_str\n",
        "\n",
        "  \n",
        "num_sentences = 10\n",
        "senten_min_length = 5\n",
        "\n",
        "for i in range(num_sentences):\n",
        "    sent = []\n",
        "    # We want long sentences, not sentences with one or two words\n",
        "    while len(sent) < senten_min_length:\n",
        "        sent = generate_sentence(model)\n",
        "    print \" \".join(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcxLArWB0wsP",
        "colab_type": "text"
      },
      "source": [
        "## Exercícios\n",
        "\n",
        "1. Treine o modelo com diferentes configurações no número de neurônios escondidos\n",
        "\n",
        "2. Aumente a quantidade de épocas e o tamanho do arquivo de treinamento e verifique se o erro diminuiu.\n"
      ]
    }
  ]
}